# Depth Estimation for a Single Omnidirectional Image with Reversed-gradient Warming-up Thresholds Discriminator

This is for ICASSP submission


![image](Overview/proposed_architecture.png)


### Encoder-decoder Model. 

The encoder-decoder model is the U-Net model. For encoder, EfficientNet B5 [17] is used as backbone because of the better performance according to our experimental results for comparing backbone of ResNet [18], EfficientNet, and DenseNet [19]. This is because EfficientNet can integrate width, depth, and resolution into a comprehensive task of the network [17]. For the decoder, we use a shallow decoder that contains two convolution layers and four bilinear upsampling layers. The encoder-decoder model takes omnidirectional RGB images as inputs and outputs corresponding feature vectors for the transformer encoder.



### Transformer and Adaptive Bins
There is an advantage in posing a regression problem as a classification problem with quantised targets. This is justified because classification problems are generally easier to solve. In this context, we transform the depth regression task into a classified task. The main body of the attentional mechanism is a vision transformer [20] based structural block that divides the depth range into multiple bins, and the central value of each bin shows the depth adaptively. 

The transformer can help as it considers global information. First, the depth range (D = (d<sub>min</sub>, d<sub>max</sub>)) of each scene will be divided into multiple small ranges (bins). Second, with the help of the attention mechanism, the centres of bins will be predicted by the transformer block according to the whole image. The final depth estimate of each pixel is a linear accumulation of these bin centres. Following this idea, depth estimation is improved by the global information provided by this vision transformer block. Details were added to the GitHub page and this will be briefly introduced in the camera-ready version.
	
In detail, the output of the encoder-decoder model will go into two branches: range attention maps and bins. The outputs from the encoder-decoder are transferred to an output map with the shape of [bs, bin_size, 128, 256] after convolution with a 3×3 kernel. 'bs' means batch size during the training process, while the 'bin_size' shows how many intervals that the depth range has been separated. This output map is then multiplied by the convolution result of the output of the transformer encoder with a 1×1 kernel to produce a range attention map. For the other branch, the encoder-decoder output goes through an embedding convolution layer and generates patch embeddings as input to the transformer encoder. The output of this encoder is generated by bins of shape [bs, bin_size, 1, 1] through a multi-layer perceptron. These bins are then multiplied by the range attention map to the depth map output.


### Reverse Gradient Warming-up Threshold Discriminator. 

As the main contribution of our work, Reverse-gradient Warming-up Threshold Discriminator (RWTD) enables the architecture to predict depth maps without training on realworld ground truths, but only on CG dataset. The discriminator in the proposed architecture is to classify output feature vectors of the encoder-decoder model from the source domain or target domain. With the idea of reverse-gradient descent [10], the RWTD is trained to be unable to distinguish which domain the feature vectors belong to. In addition, RWTD allows the discriminator to focus on similar images while ignoring the differentiated ones from the source and target domains with the increase of epoch number. In this way, compared with the previous GAN-based domain adaptation methods [10, 21, 7], it assigns different weights to different scenes in the training data set during the training process. Therefore, the information learned in the source domain can be applied to predict depth maps of unlabelled scenes from the target domain. Moreover, a further reason why the previous architecture cannot train just on CG pictures and predict depth maps for real-world situations is that the domain label losses will continue to increase and dominate the loss function, hence guiding the whole architecture in the incorrect gradient direction. To address this issue, RWTD employs warming-up thresholds to set constraints on the loss values throughout the training process, and this value is modified based on the training epoch to ensure the optimal performance of the whole architecture (details shown in Sec. 3.2.1).



## Dataset

Download the 3D60 depth dataset from https://vcl3d.github.io/3D60/ 
```
@inproceedings{zioulis2018omnidepth,
	title={Omnidepth: Dense depth estimation for indoors spherical panoramas},
	author={Zioulis, Nikolaos and Karakottas, Antonis and Zarpalas, Dimitrios and Daras, Petros},
	booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
	pages={448--465},
	year={2018}
}
```

### Data Exploration

The CG images are from SunCG, and two different office-scene-based real-world 360 image datasets of two different buildings from Stanford2D3D. These datasets contain 512 $\times$ 256 resolution RGB images of indoor scenes with corresponding depth maps in metres. The real-world ground truth depth maps contain outliers caused by missing depth pixels. In order to improve the training efficiency, the scenes containing over 5% outliers are removed. After the pre-processing, SunCG contains 2319 scenes. Stanford2D3D area5 contains 82 scenes, and area6 contains 132 scenes. 

<img src="Materials/SunCG2.png" width="600px">

Figure above shows sampled scenes of the SunCG dataset. They illustrate RGB images and corresponding depth maps of indoor scenes that are simulated and rendered by computers. SunCG contains different scenes that cover a variety of objects that might exist in the real world, such as beds, ladders, fans, etc. In some scenes, it also includes simulated humans. 

<img src="Materials/area6.png" width="600px">

However, there are some differences between these CG and real-world scenes, including the textures and colours of the scenes. Figure above shows the real-world scenes from the Stanford2D3D dataset, which are taken in different buildings. It also includes many images, but they are from very limited kinds of scenes. 


## Loss Function
The loss function combines the dense depth loss, the ChamferLoss, and Domain Label Loss (DLL) (Equation \ref{f1}). $\alpha$  and $\beta$  represent the factor of dense depth and ChamferLoss, respectively. $\delta$ represents Domain Label Loss Factor (DLLF). It controls the increased speed of DLL. These factors balance the weight of different losses and lead to the good performance of the proposed architecture. 

### Dense Loss 
We use a Scale-invariant (SI) Loss for the dense depth loss function. In contrast to the square variance error, which usually measures the difference between two images, SI Loss does not depend on the scale of the images. 
	
### Chamfer Loss
In order to shrink the gap between bin centre and ground truth depth values, we use the Chamfer loss function, which uses bi-directional chamfering losses as a regularised item. This loss can make the bin centres to be close to the depth values of ground truths while making the rest to be far from these depth values. 

### Domain Label Losses
See Sec 3.2.1 

## Performance on New Dataset

![image](Materials/depth_comparison.png)

The figure above shows real-world images captured in a building with an off-the-shelf omnidirectional camera. Though there is no ground-truth depth data, it can be observed that the estimated depth maps show the correct depth of the scenes with smooth changes within objects. Compared with the results from AdaBins, the proposed method shows better depth estimation for the planar ceiling regions in all test images. As AdaBins are affected by textures and lighting conditions, the saturated areas by the lighting in the ceiling show the wrong depth, while the proposed RWTD produced smooth and planar ceiling regions learned from scenes in the source domain.  


## Install

Check https://pytorch.org/ and choose a suitable version of PyTorch for your computers/servers

```
pip install opencv-contrib-python pytorch3d
```

## Train
### complete the argumentparser (fill with absolute path)

The default parameters have been set except:
--filenames_file: source domain dataset (.txt file)
--filenames_file_eval: target domain dataset (.txt file)


### start training 
```
python train_DA_RWTD.py --bs 16 --epochs 100 --comments your_comments
```


## Appendix

<table align="center" style="width:100%; border:#000 solid; border-width:1px 0">
<caption>Table 2: Effect of discriminator</caption>
	
<thead style="border-bottom:#000 1px solid;">
<tr>
<th style="border:0">Model</th>
<td style="border:0">a1</td>
<td style="border:0">a2</td>
<td style="border:0">a3</td>
<td style="border:0">rel</td>
<td style="border:0">rms</td>
<td style="border:0">log10</td>
</tr>
</thead>
<tr>
<th style="border:0">Unsupervised DA [7]</th>
<td style="border:0">26.2</td>
<td style="border:0">50.7</td>
<td style="border:0">68.1</td>
<td style="border:0">0.855</td>
<td style="border:0">1.720</td>
<td style="border:0">0.235</td>
</tr>
<tr>
<th style="border:0">RWTD (Ours)</th>
<td style="border:0">74.08±2.37</td>
<td style="border:0">95.81±0.63</td>
<td style="border:0">99.21±0.2</td>
<td style="border:0">0.18±0.009</td>
<td style="border:0">0.543±0.042</td>
<td style="border:0">0.069±0.003</td>
</table>



<table align="center" style="width:100%; border:#000 solid; border-width:1px 0">
<caption>Table 3: Investigation on the effect of each component in the proposed architecture</caption>
	
<thead style="border-bottom:#000 1px solid;">
<tr>
<th style="border:0">Model</th>
<td style="border:0">a1</td>
<td style="border:0">a2</td>
<td style="border:0">a3</td>
<td style="border:0">rel</td>
<td style="border:0">rms</td>
<td style="border:0">log10</td>
</tr>
</thead>
<tr>
<th style="border:0">Encoder-decoder model only</th>
<td style="border:0">65.15±4.05</td>
<td style="border:0">91.13±1.59</td>
<td style="border:0">97.71±0.53</td>
<td style="border:0">0.24±0.025</td>
<td style="border:0">0.683±0.055</td>
<td style="border:0">0.087±0.008</td>
</tr>
<tr>
<th style="border:0">with RD</th>
<td style="border:0">69.68±5.43</td>
<td style="border:0">94.57±1.95</td>
<td style="border:0">99.03±0.4</td>
<td style="border:0">0.199±0.025</td>
<td style="border:0">0.565±0.068</td>
<td style="border:0">0.075±0.008</td>
</tr>
<tr>
<th style="border:0">with RWTD (Ours) </th>
<td style="border:0">74.08±2.37 </td>
<td style="border:0">95.81±0.63</td>
<td style="border:0">99.21±0.2</td>
<td style="border:0">0.18±0.009</td>
<td style="border:0">0.543±0.042</td>
<td style="border:0">0.069±0.003</td>
</table>




<table align="center" style="width:100%; border:#000 solid; border-width:1px 0">
<caption>Table: Overfitted model</caption>
	
<thead style="border-bottom:#000 1px solid;">
<tr>
<th style="border:0">Testing Dataset</th>
<td style="border:0">a1</td>
<td style="border:0">a2</td>
<td style="border:0">a3</td>
<td style="border:0">rel</td>
<td style="border:0">rms</td>
<td style="border:0">log10</td>
</tr>
</thead>
<tr>
<th style="border:0">SUNCG Training</th>
<td style="border:0">0.9868</td>
<td style="border:0">0.9976</td>
<td style="border:0">0.9993</td>
<td style="border:0">0.0344</td>
<td style="border:0">0.2316</td>
<td style="border:0">0.0150</td>
</tr>
<tr>
<th style="border:0">SUNCG Testing</th>
<td style="border:0">0.9451</td>
<td style="border:0">0.9844</td>
<td style="border:0">0.9949</td>
<td style="border:0">0.0547</td>
<td style="border:0">0.3705</td>
<td style="border:0">0.0244</td>
</table>



For example, RectNet was trained with SUNCG training dataset and tested with SUNCG training and testing datasets, respectively. The result of it with LR=0.0001 and a1=0.3651 for the real-world dataset is shown in Table 1. 


## Reference

```
@article{alhashim2018high,
  title={High quality monocular depth estimation via transfer learning},
  author={Alhashim, Ibraheem and Wonka, Peter},
  journal={arXiv preprint arXiv:1812.11941},
  year={2018}
}

@inproceedings{bhat2021adabins,
  title={Adabins: Depth estimation using adaptive bins},
  author={Bhat, Shariq Farooq and Alhashim, Ibraheem and Wonka, Peter},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4009--4018},
  year={2021}
}
```
